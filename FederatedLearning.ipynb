{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNFWENyQvlmC3q6dEg92i1K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aWolander/Cryptography/blob/main/FederatedLearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jebBFX_PmOqW"
      },
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "import copy\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Subset\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import numpy as np\n",
        "# vits16 = torch.hub.load('facebookresearch/dino:main', 'dino_vits16')\n",
        "\n",
        "# print(vits16)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    K= 2\n",
        "    N = 10\n",
        "    CIFAR100 = datasets.CIFAR100(\n",
        "      root=\"data\",\n",
        "      download=True,\n",
        "      transform=ToTensor()\n",
        "    )\n",
        "    train_dataset, validate_dataset, test_dataset = torch.utils.data.random_split(CIFAR100, [0.7,0.15,0.15])\n",
        "    client_datasets = split_data_non_iid(train_dataset, K, N)\n",
        "    test_split(client_datasets)\n",
        "\n",
        "def create_label_indexing(dataset):\n",
        "    label_index = {i: [] for i in range(100)}\n",
        "    for idx, (_, label) in enumerate(dataset):\n",
        "        label_index[label].append(idx)\n",
        "    return label_index\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "def split_data_non_iid(dataset, K, N_c=20, seed=42):\n",
        "    # as far I can tell N_c does exactly nothing\n",
        "    random.seed(seed)\n",
        "    label_index = create_label_indexing(dataset)\n",
        "    client_indices = [[] for _ in range(K)]\n",
        "\n",
        "    class_to_clients = {label: set() for label in range(100)} # why sets?\n",
        "    for label in range(100):\n",
        "        selected_clients = random.sample(range(K), k=max(1, K // 5)) # why define variable k? why K // 5?\n",
        "        class_to_clients[label].update(selected_clients)\n",
        "\n",
        "    # why are these two for loops?\n",
        "    client_to_classes = {client: set() for client in range(K)} # why do you need both client_to_classes and class_to_clients?\n",
        "    for label in range(100):\n",
        "        for client in class_to_clients[label]:\n",
        "            if len(client_to_classes[client]) < N_c:\n",
        "                client_to_classes[client].add(label)\n",
        "\n",
        "    # what\n",
        "    all_labels = list(range(100)) # ???\n",
        "    for client in range(K):\n",
        "        while len(client_to_classes[client]) < N_c:\n",
        "            label = random.choice(all_labels)\n",
        "            client_to_classes[client].add(label) # is this dict even used?\n",
        "            class_to_clients[label].add(client)\n",
        "\n",
        "    for label, indices in label_index.items():\n",
        "        random.shuffle(indices) # why? this is done at the end\n",
        "        clients = list(class_to_clients[label])\n",
        "        num_clients = len(clients)\n",
        "\n",
        "        # im pretty sure this is just the split(l,n) function\n",
        "        split_size = len(indices) // num_clients\n",
        "        for i, client in enumerate(clients):\n",
        "            start = i * split_size\n",
        "            end = (i + 1) * split_size if i < num_clients - 1 else len(indices)\n",
        "            client_indices[client].extend(indices[start:end])\n",
        "\n",
        "    for indices in client_indices:\n",
        "        random.shuffle(indices)\n",
        "\n",
        "    return [Subset(dataset, indices) for indices in client_indices]\n",
        "'''\n",
        "\n",
        "def split(l: list, n: int) -> list[list]:\n",
        "    '''splits a list into n parts'''\n",
        "    split_list = []\n",
        "    for i in range(0, n):\n",
        "        split_list.append(l[i::n])\n",
        "    return split_list\n",
        "\n",
        "def split_data_iid(dataset, K):\n",
        "    index_split = [[] for x in range(K)]\n",
        "    label_index = create_label_indexing(dataset)\n",
        "\n",
        "    for label in range(0,100):\n",
        "        split_indices = split(label_index[label],K)\n",
        "        for client in range(0, K):\n",
        "            index_split[client] += split_indices[client]\n",
        "\n",
        "    for client_indices in index_split:\n",
        "        random.shuffle(client_indices)\n",
        "    return [Subset(dataset, indices) for indices in index_split]\n",
        "\n",
        "def test_split(client_datasets):\n",
        "    bottom = np.zeros(100)\n",
        "    for client_id, client_dataset in enumerate(client_datasets):\n",
        "        occurrences = np.zeros(100)\n",
        "        for datapoint in client_dataset:\n",
        "            label = datapoint[1]\n",
        "            occurrences[label] += 1\n",
        "        #print(occurrences)\n",
        "        print(f\"Client {client_id}: No. of nonzero elements: {np.count_nonzero(occurrences)} | Avg., stdev. of nonzero elements: {occurrences[occurrences>0].mean()}, {occurrences[occurrences>0].std()}\") # should be N_c non-zero elements!\n",
        "        plt.bar(range(100), occurrences, bottom=bottom, label=client_id)\n",
        "        plt.xlabel(\"Class label\")\n",
        "        plt.ylabel(\"Number of samples\")\n",
        "        bottom += occurrences\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "main()\n",
        "\n",
        "# def test_split_index(dataset):\n",
        "#     K= 2\n",
        "#     N = 5\n",
        "#     client_data_split = split_data_non_iid(dataset, K, N)\n",
        "#     for client_data in client_data_split:\n",
        "#         occurences = {}\n",
        "#         for index in client_data:\n",
        "#             occurences[dataset[index][1]] = occurences.setdefault(dataset[index][1], 0) + 1\n",
        "#         print(occurences)\n",
        "#         plt.hist(occurences, stacked=True, bins=100)\n",
        "#     plt.show()\n"
      ],
      "metadata": {
        "id": "_r1A6pjOmh85"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
        "class DinoFullModel(nn.Module):\n",
        "    def __init__(self):\n",
        "      super().__init__()\n",
        "      # Load the full DINO model (backbone + head)\n",
        "      self.model = torch.hub.load('facebookresearch/dino:main', 'dino_vits16').to(device)\n",
        "\n",
        "      self.learning_rate = 1e-2\n",
        "      self.epochs = 10\n",
        "\n",
        "      self.loss_fn = nn.CrossEntropyLoss()\n",
        "      self.optimizer = torch.optim.SGD(self.parameters(), lr=self.learning_rate)\n",
        "\n",
        "      self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "        self.optimizer, T_max=self.epochs\n",
        "      )\n",
        "\n",
        "    def forward(self, x):\n",
        "      return self.model(x)\n",
        "\n",
        "    def train_model(self, dataloader):\n",
        "      size = len(dataloader.dataset)\n",
        "      # Set the model to training mode - important for batch normalization and dropout layers\n",
        "      # Unnecessary in this situation but added for best practices\n",
        "      self.train()\n",
        "      for epoch in range(self.epochs):\n",
        "        current = 0\n",
        "        print(f\"-------------------------------\\nEpoch {epoch+1}\\n-------------------------------\")\n",
        "        for (X, y) in dataloader:\n",
        "          # Compute prediction and loss\n",
        "          X = X.to(device)\n",
        "          y = y.to(device)\n",
        "          pred = self.model(X).to(device)\n",
        "          loss = self.loss_fn(pred, y)\n",
        "\n",
        "          # Backpropagation\n",
        "          loss.backward()\n",
        "          self.optimizer.step()\n",
        "          self.optimizer.zero_grad()\n",
        "          current += len(X)\n",
        "          if current % 5000 == 0:\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "            loss = loss.item()\n",
        "\n",
        "        self.scheduler.step()\n",
        "\n",
        "\n",
        "\n",
        "    def test_model(self, dataloader):\n",
        "      self.model.eval()\n",
        "      total_loss = 0.0\n",
        "      total_correct = 0\n",
        "      total_samples = 0\n",
        "\n",
        "      with torch.no_grad():\n",
        "        for inputs, targets in dataloader:\n",
        "          inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "          outputs = self.model(inputs)\n",
        "          loss = self.loss_fn(outputs, targets)\n",
        "          total_loss += loss.item() * inputs.size(0)  # total loss, not average\n",
        "\n",
        "          # Get predicted class\n",
        "          preds = outputs.argmax(dim=1)\n",
        "          total_correct += (preds == targets).sum().item()\n",
        "          total_samples += targets.size(0)\n",
        "\n",
        "      avg_loss = total_loss / total_samples\n",
        "      accuracy = total_correct / total_samples\n",
        "\n",
        "      #print(f\"Validation Loss: {avg_loss:.4f}, Correct: {total_correct} out of {len(dataloader)}, Accuracy: {accuracy:.2%}\")\n",
        "\n",
        "      return avg_loss, total_correct, accuracy\n",
        "\n",
        "def main():\n",
        "\n",
        "  train_dataset, validate_dataset, test_dataset = torch.utils.data.random_split(CIFAR100, [0.7,0.15,0.15])\n",
        "\n",
        "  train_dataloader = DataLoader(train_dataset, batch_size=100)\n",
        "  test_dataloader = DataLoader(test_dataset, batch_size=100)\n",
        "  validate_dataloader = DataLoader(validate_dataset, batch_size=100)\n",
        "\n",
        "  model = DinoFullModel().to(device)\n",
        "  #for param in model.parameters():\n",
        "  #    print(param.shape)\n",
        "\n",
        "  model.train_model(train_dataloader)\n",
        "  model.test_model(test_dataloader)\n",
        "  print(\"Done!\")\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "  main()"
      ],
      "metadata": {
        "id": "_KsMK9MMmkEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FL_client(DinoFullModel):\n",
        "  '''\n",
        "  Basically just the single model, but with addition and scaling functionality.\n",
        "  To facilitate FedAvg more easily.\n",
        "  '''\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "  def __add__(self, other: FL_client) -> FL_client:\n",
        "    assert isinstance(other, FL_client)\n",
        "    temp_weights = []\n",
        "    temp_biases = []\n",
        "    for (name, self_param), (_,other_param) in zip(self.named_parameters(), other.named_parameters()):\n",
        "      self_param.to(device)\n",
        "      other_param.to(device)\n",
        "      if 'weight' in name:\n",
        "        temp_weights.append(self_param + other_param)\n",
        "      elif 'bias' in name:\n",
        "        temp_biases.append(self_param + other_param)\n",
        "    temp_client = copy.deepcopy(self) # pointers??\n",
        "    with torch.no_grad():\n",
        "      i = 0\n",
        "      for name, param in temp_client.model.named_parameters():\n",
        "        if 'weight' in name:\n",
        "          param.copy_(temp_weights[i])\n",
        "          i += 1\n",
        "\n",
        "      j = 0\n",
        "      for name, param in temp_client.model.named_parameters():\n",
        "        if 'bias' in name:\n",
        "          param.copy_(temp_biases[j])\n",
        "          j += 1\n",
        "    # order?\n",
        "    return temp_client\n",
        "\n",
        "  def __mul__(self, multiplier: float|int) -> FL_client:\n",
        "    assert isinstance(multiplier, float|int)\n",
        "    temp_weights = []\n",
        "    temp_biases = []\n",
        "    for (name, self_param) in self.named_parameters():\n",
        "      self_param.to(device)\n",
        "      if 'weight' in name:\n",
        "        temp_weights.append(self_param * multiplier)\n",
        "      elif 'bias' in name:\n",
        "        temp_biases.append(self_param * multiplier)\n",
        "\n",
        "    temp_client = copy.deepcopy(self)\n",
        "    #order?\n",
        "    with torch.no_grad():\n",
        "      i = 0\n",
        "      for name, param in temp_client.model.named_parameters():\n",
        "        if 'weight' in name:\n",
        "          param.copy_(temp_weights[i])\n",
        "          i += 1\n",
        "\n",
        "      j = 0\n",
        "      for name, param in temp_client.model.named_parameters():\n",
        "        if 'bias' in name:\n",
        "          param.copy_(temp_biases[j])\n",
        "          j += 1\n",
        "    return temp_client\n",
        "\n",
        "  def __rmul__(self, multiplier: float|int) -> FL_client:\n",
        "    return self.__mul__(multiplier)\n",
        "\n",
        "  def __sub__(self, other: FL_client) -> FL_client:\n",
        "    assert isinstance(other, FL_client)\n",
        "    return self + (-other)\n",
        "\n",
        "\n",
        "\n",
        "class FL_server():\n",
        "  def __init__(self, K):\n",
        "    self.model = torch.hub.load('facebookresearch/dino:main', 'dino_vits16').to(device)\n",
        "\n",
        "    self.K = K\n",
        "    self.training_rounds = 10\n",
        "\n",
        "    self.loss_fn = nn.CrossEntropyLoss()\n",
        "    self.clients = []\n",
        "    for i in range(self.K):\n",
        "      self.clients.append(FL_client()) # pointers?\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.model(x)\n",
        "\n",
        "  def FedAvg(self, dataloaders_list, validate_dataloader, C):\n",
        "    for round in range(self.training_rounds):\n",
        "      m = max(C*self.K, 1)\n",
        "      rand_set_clients = random.sample(range(self.K), m)\n",
        "      client_model_sum = None\n",
        "      m_t = 0\n",
        "      for client_id in rand_set_clients:\n",
        "        temp_client = self.clients[client_id]\n",
        "        temp_loader = dataloaders_list[client_id]\n",
        "\n",
        "        print(f\"----------- TRAIN LOOP FOR CLIENT {client_id}. -----------\")\n",
        "        temp_client.train_model(temp_loader)\n",
        "        n_k = len(temp_loader.dataset)\n",
        "        m_t += n_k\n",
        "        # Easier than instantiating an empty model\n",
        "        if client_model_sum is None:\n",
        "          client_model_sum = n_k*temp_client\n",
        "        else:\n",
        "          client_model_sum = client_model_sum + n_k*temp_client\n",
        "\n",
        "      client_model_sum = (1/m_t) * client_model_sum\n",
        "      self.model = client_model_sum.model\n",
        "      print(f\"----------- VALIDATION FOR SERVER. -----------\")\n",
        "      self.test_model(validate_dataloader)\n",
        "\n",
        "\n",
        "  def test_model(self, dataloader):\n",
        "    self.model.eval()\n",
        "    total_loss = 0.0\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for inputs, targets in dataloader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        outputs = self.model(inputs)\n",
        "        loss = self.loss_fn(outputs, targets)\n",
        "        total_loss += loss.item() * inputs.size(0)  # total loss, not average\n",
        "\n",
        "        # Get predicted class\n",
        "        preds = outputs.argmax(dim=1)\n",
        "        total_correct += (preds == targets).sum().item()\n",
        "        total_samples += targets.size(0)\n",
        "\n",
        "    avg_loss = total_loss / total_samples\n",
        "    accuracy = total_correct / total_samples\n",
        "\n",
        "    print(f\"Validation Loss: {avg_loss:.4f}, Correct: {total_correct} out of {len(dataloader)}, Accuracy: {accuracy:.2%}\")\n",
        "\n",
        "    return avg_loss, total_correct, accuracy\n",
        "\n"
      ],
      "metadata": {
        "id": "dWrl3Z95mnm5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "TODO:\n",
        "Batch normalization, probably. I think I read somewhere that the facebook model needs this\n",
        "Version control and checkpointing\n",
        "Testing and hyperparameter tuning\n",
        "gradient mask TaLoS thing. Also expanding this to FL.\n",
        "Plot accuracy and loss method in DinoFullModel\n",
        "'''\n",
        "\n",
        "K=5\n",
        "FL = FL_server(K)\n",
        "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
        "CIFAR100 = datasets.CIFAR100(\n",
        "  root=\"data\",\n",
        "  download=True,\n",
        "  transform=ToTensor()\n",
        ")\n",
        "train_dataset, validate_dataset, test_dataset = torch.utils.data.random_split(CIFAR100, [0.7,0.15,0.15])\n",
        "\n",
        "\n",
        "split_dataset = split_data_iid(train_dataset, K)\n",
        "\n",
        "train_dataloader = [DataLoader(subset, batch_size=100) for subset in split_dataset]\n",
        "\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=100)\n",
        "validate_dataloader = DataLoader(validate_dataset, batch_size=100)\n",
        "\n",
        "FL.FedAvg(train_dataloader, validate_dataloader, 1)\n",
        "FL.test_model(test_dataloader)\n",
        "\n"
      ],
      "metadata": {
        "id": "svTbWrfqmqAx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}